{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pre-trained model on NIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('climabench/miniLM-cdp-all')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qs = (\n",
    "    pd.read_csv('CDP/Cities/Cities Responses/combined.csv', low_memory=False, usecols=['Question Name'])['Question Name']\n",
    "    .unique().tolist()\n",
    ")\n",
    "\n",
    "num_qs = len(all_qs)\n",
    "print(num_qs)\n",
    "\n",
    "q_to_index = {q: i for i, q in enumerate(all_qs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(response: str, outputs, true_q: str | None = None) -> None:\n",
    "    top5_ids = outputs.argsort(descending=True)[:5]\n",
    "    top5_qs = {round(torch.sigmoid(outputs[i]).item(), 4): all_qs[i] for i in top5_ids}\n",
    "    print(f'Response:\\t{response}')\n",
    "    if true_q is not None:\n",
    "        print(f'True Q:\\t\\t{true_q}')\n",
    "    pprint(top5_qs, width=500, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = pd.read_csv('nist/AlamedaCA_carp_final_091119.csv', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for passage in passages:\n",
    "    batch = tokenizer(all_qs, [passage] * num_qs, padding='longest', truncation=True,\n",
    "                      return_tensors='pt', max_length=512, return_token_type_ids=True)\n",
    "    for k in ('input_ids', 'attention_mask', 'token_type_ids'):\n",
    "        batch[k] = batch[k].to(device, non_blocking=True)\n",
    "    outputs = model(**batch).logits.cpu().squeeze()\n",
    "    print('============================')\n",
    "    process_output(passage, outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('climabench2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d664b0b25f51ce5fa4fcb103afb7398d6dfb14da7a590c9b5a6e267cd752f66b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
